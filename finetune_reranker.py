# -*- coding: utf-8 -*-
"""finetune_reranker.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lm9u7dFVMSgt2kquUwGHL1qdS4CVpNC4
"""

!pip install transformers
!pip install wget

"""QBData class"""

from typing import List, Dict, Iterable, Optional, Tuple, NamedTuple
import os
import json
import random

GUESSER_TRAIN_FOLD = 'guesstrain'
BUZZER_TRAIN_FOLD = 'buzztrain'
TRAIN_FOLDS = {GUESSER_TRAIN_FOLD, BUZZER_TRAIN_FOLD}

# Guesser and buzzers produce reports on these for cross validation
GUESSER_DEV_FOLD = 'guessdev'
BUZZER_DEV_FOLD = 'buzzdev'
DEV_FOLDS = {GUESSER_DEV_FOLD, BUZZER_DEV_FOLD}

# System-wide cross validation and testing
GUESSER_TEST_FOLD = 'guesstest'
BUZZER_TEST_FOLD = 'buzztest'


class Question(NamedTuple):
    qanta_id: int
    text: str
    first_sentence: str
    tokenizations: List[Tuple[int, int]]
    answer: str
    page: Optional[str]
    fold: str
    gameplay: bool
    category: Optional[str]
    subcategory: Optional[str]
    tournament: str
    difficulty: str
    year: int
    proto_id: Optional[int]
    qdb_id: Optional[int]
    dataset: str
    answer_prompt: str = ""

    def to_json(self) -> str:
        return json.dumps(self._asdict())

    @classmethod
    def from_json(cls, json_text):
        return cls(**json.loads(json_text))

    @classmethod
    def from_dict(cls, dict_question):
        return cls(**dict_question)

    def to_dict(self) -> Dict:
        return self._asdict()

    @property
    def sentences(self) -> List[str]:
        """
        Returns a list of sentences in the question using preprocessed spacy 2.0.11
        """
        return [self.text[start:end] for start, end in self.tokenizations]

    def runs(self, char_skip: int) -> Tuple[List[str], List[int]]:
        """
        Returns runs of the question based on skipping char_skip characters at a time. Also returns the indices used

        q: name this first united states president.
        runs with char_skip=10:
        ['name this ',
         'name this first unit',
         'name this first united state p',
         'name this first united state president.']

        :param char_skip: Number of characters to skip each time
        """
        char_indices = list(range(char_skip, len(self.text) + char_skip, char_skip))
        return [self.text[:i] for i in char_indices], char_indices


class QantaDatabase:
    def __init__(self, dataset_path=os.path.join('data')):
        with open(dataset_path) as f:
            self.dataset = json.load(f)

        self.version = self.dataset['version']
        self.raw_questions = self.dataset['questions']
        self.all_questions = [Question(**q) for q in self.raw_questions]
        self.mapped_questions = [q for q in self.all_questions if q.page is not None]

        self.train_questions = [q for q in self.mapped_questions if q.fold in TRAIN_FOLDS]
        self.guess_train_questions = [q for q in self.train_questions if q.fold == GUESSER_TRAIN_FOLD]
        self.buzz_train_questions = [q for q in self.train_questions if q.fold == BUZZER_TRAIN_FOLD]

        self.dev_questions = [q for q in self.mapped_questions if q.fold in DEV_FOLDS]
        self.guess_dev_questions = [q for q in self.dev_questions if q.fold == GUESSER_DEV_FOLD]
        self.buzz_dev_questions = [q for q in self.dev_questions if q.fold == BUZZER_DEV_FOLD]

        self.buzz_test_questions = [q for q in self.mapped_questions if q.fold == BUZZER_TEST_FOLD]
        self.guess_test_questions = [q for q in self.mapped_questions if q.fold == GUESSER_TEST_FOLD]

    def by_fold(self):
        return {
            GUESSER_TRAIN_FOLD: self.guess_train_questions,
            GUESSER_DEV_FOLD: self.guess_dev_questions,
            BUZZER_TRAIN_FOLD: self.buzz_train_questions,
            BUZZER_DEV_FOLD: self.buzz_dev_questions,
            BUZZER_TEST_FOLD: self.buzz_test_questions,
            GUESSER_TEST_FOLD: self.guess_test_questions
        }


class QuizBowlDataset:
    def __init__(self, *, guesser_train=False, buzzer_train=False):
        """
        Initialize a new quiz bowl data set
        """
        super().__init__()
        if not guesser_train and not buzzer_train:
            raise ValueError('Requesting a dataset which produces neither guesser or buzzer training data is invalid')

        if guesser_train and buzzer_train:
            print('Using QuizBowlDataset with guesser and buzzer training data, make sure you know what you are doing!')

        self.db = QantaDatabase()
        self.guesser_train = guesser_train
        self.buzzer_train = buzzer_train

    def training_data(self):
        training_examples = []
        training_pages = []
        questions = []
        if self.guesser_train:
            questions.extend(self.db.guess_train_questions)
        if self.buzzer_train:
            questions.extend(self.db.buzz_train_questions)

        for q in questions:
            training_examples.append(q.sentences)
            training_pages.append(q.page)

        return training_examples, training_pages, None

    def questions_by_fold(self):
        return {
            GUESSER_TRAIN_FOLD: self.db.guess_train_questions,
            GUESSER_DEV_FOLD: self.db.guess_dev_questions,
            BUZZER_TRAIN_FOLD: self.db.buzz_train_questions,
            BUZZER_DEV_FOLD: self.db.buzz_dev_questions,
            BUZZER_TEST_FOLD: self.db.buzz_test_questions,
            GUESSER_TEST_FOLD: self.db.guess_test_questions
        }

    def questions_in_folds(self, folds):
        by_fold = self.questions_by_fold()
        questions = []
        for fold in folds:
            questions.extend(by_fold[fold])
        return questions


class WikiLookup:
    def __init__(self, filepath:str) -> None:
        with open(filepath) as fp:
            self.page_lookup = json.load(fp)
        self.passage_list = list(self.page_lookup.items())
    
    def __getitem__(self, page):
        """Return the page content as python dict with key `text`. 
        If the page is not found, it only returns a human readable title of the page."""
        return self.page_lookup.get(page, {'text': page.replace('_', ' ')})

    def get_random_passage_masked(self):
      raw = random.choice(self.passage_list)
      title = raw[0].replace("_", " ")
      text = raw[1]['text']
      masked_text = text.replace(title, " ")
      return masked_text

    def get_random_passage_raw(self):
      return random.choice(self.passage_list)[1]['text']

from typing import List, Union
!pip install transformers datasets

from transformers import DataCollatorWithPadding, BertTokenizer
import datasets
import pickle
from torch.utils.data import DataLoader
from transformers import AdamW
from transformers import get_scheduler
from transformers import TrainingArguments
from transformers import Trainer

from tqdm.auto import tqdm

import torch
from torch.optim import AdamW, optimizer, lr_scheduler
from transformers import BertForSequenceClassification, pipeline, BertModel, BertConfig
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForSequenceClassification

data_path = "/content/drive/MyDrive/848-hw-main/data"
model_path = "/content/drive/MyDrive/848-hw-main/hw2/models"
wiki_path = "/content/drive/MyDrive/848-hw-main/data/wiki_lookup.2018.json"

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

from google.colab import drive
drive.mount('/content/drive')

from transformers import BertForSequenceClassification, pipeline, BertModel, BertConfig
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForSequenceClassification

#qanta_db_train = QantaDatabase(data_path+'/small.guesstrain.json')
#qanta_db_dev = QantaDatabase(data_path+'/small.guessdev.json')

qanta_db_train = QantaDatabase(data_path+'/small.guesstrain.json')
qanta_db_dev = QantaDatabase(data_path+'/small.guessdev.json')
wiki_data = WikiLookup(wiki_path)

training_data = qanta_db_train
validation_data = qanta_db_dev

model_identifier = 'amberoad/bert-multilingual-passage-reranking-msmarco'
max_model_length = 512
tokenizer = AutoTokenizer.from_pretrained(model_identifier, model_max_length=max_model_length)
model = AutoModelForSequenceClassification.from_pretrained(model_identifier, num_labels=2)

def preprocess_function(data):
  return tokenizer(data["questions"], data["content"], truncation=True)

questions = [x.text for x in training_data.guess_train_questions]
answers = [x.page for x in training_data.guess_train_questions]
randints = [random.randrange(0,3) for i in range(len(answers))] #list of len(answers) random numbers from [0,1,2] 
print(randints)

passages = [wiki_data[x.page]['text'].replace(x.page.replace('_',' '), ' ') if randints[i]<2 else wiki_data.get_random_passage_masked() for i,x in enumerate(training_data.guess_train_questions)]
labels = [1 if randints[i]<2 else 0 for i in randints]

#print(labels[:5])

#for i in range(5):
#  print(questions[i])
#  print(passages[i])
#  print(labels[i])

#replaced page title in text with space
#with prob 1/3 take random negative samples from wiki_data


data_train = datasets.Dataset.from_dict({'questions':questions,  'labels':labels, 'content':passages})

print(data_train)

questions = [x.text for x in validation_data.guess_train_questions]
answers = [x.page for x in validation_data.guess_train_questions]
randints = [random.randrange(0,3) for i in range(len(answers))] #list of len(answers) random numbers from [0,1,2] 
print(randints)

passages = [wiki_data[x.page]['text'].replace(x.page.replace('_',' '), ' ') if randints[i]<2 else wiki_data.get_random_passage_masked() for i,x in enumerate(validation_data.guess_train_questions)]
labels = [1 if randints[i]<2 else 0 for i in randints]

#print(labels[:5])

#for i in range(5):
#  print(questions[i])
#  print(passages[i])
#  print(labels[i])

#replaced page title in text with space
#with prob 1/3 take random negative samples from wiki_data


data_dev = datasets.Dataset.from_dict({'questions':questions,  'labels':labels, 'content':passages})
print(data_dev)

data = datasets.DatasetDict({'train': data_train, 'validation': data_dev})
print(data)

raw_train_dataset = data["train"]
print(raw_train_dataset[0])

tokenized_dataset = data.map(preprocess_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
print(tokenized_dataset)




try:
  tokenized_dataset = tokenized_dataset.remove_columns(["questions", "content"])
except:
  pass
tokenized_dataset.set_format("torch")
tokenized_dataset["train"].column_names

from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_dataset["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_dataset["validation"], batch_size=8, collate_fn=data_collator
)

for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}

outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
!pip install accelerate

import torch
from tqdm.auto import tqdm


from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

with open(model_path, 'wb') as f:
    pickle.dump({
        'reranker-finetuned': model
    }, f)



"""Training the BERT based guesser"""

